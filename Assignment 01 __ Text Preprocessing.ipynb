{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests as rq\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### downloading From API and applying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "\n",
    "# url = 'https://api.themoviedb.org/3/genre/movie/list?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US'\n",
    "# response = rq.get(url)\n",
    "\n",
    "\n",
    "# genre_ids = response.json()\n",
    "# genre_ids = genre_ids['genres']\n",
    "# genre_dict = {genre['id']: genre['name'] for genre in genre_ids}\n",
    "\n",
    "# def map_genre_ids(genre_ids_list):\n",
    "#     return [genre_dict[genre_id] for genre_id in genre_ids_list]\n",
    "\n",
    "# def map_genre_ids_to_string(genre_ids_list):\n",
    "#     genre_names = [genre_dict[genre_id] for genre_id in genre_ids_list]\n",
    "#     return ', '.join(genre_names)\n",
    "\n",
    "\n",
    "# for i in range (1, 400):\n",
    "    \n",
    "#     url = f\"https://api.themoviedb.org/3/movie/top_rated?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US&page={i}\"\n",
    "\n",
    "#     headers = {\n",
    "#         \"accept\": \"application/json\",\n",
    "#         \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyYzNkYTc2YTE1OTZmYTdmZWRjNDhkYTQ3YzViNTI1MCIsInN1YiI6IjY2MmQzYmIxMDNiZjg0MDEyMmVhNWZjYiIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.e9XR2cubnlD0gHLS-qGZGD7LBpZwciPCq-NzixlDZJM\"\n",
    "#     }\n",
    "\n",
    "#     response = rq.get(url, headers=headers)\n",
    "#     temp = pd.DataFrame(response.json()['results'])[[\n",
    "#         'adult', 'backdrop_path', 'genre_ids', 'id', 'original_language', 'original_title', 'overview', 'popularity', 'poster_path', 'release_date', 'title', 'video', 'vote_average', 'vote_count'\n",
    "#     ]]\n",
    "#     temp['genre_names'] = temp['genre_ids'].apply(map_genre_ids_to_string)\n",
    "#     df = pd.concat([df, temp], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Movie_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>adult</th>\n",
       "      <th>backdrop_path</th>\n",
       "      <th>genre_ids</th>\n",
       "      <th>id</th>\n",
       "      <th>original_language</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>poster_path</th>\n",
       "      <th>release_date</th>\n",
       "      <th>title</th>\n",
       "      <th>video</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "      <th>genre_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>/zfbjgQE1uSd9wiPTX4VzsLi0rGG.jpg</td>\n",
       "      <td>[18, 80]</td>\n",
       "      <td>278</td>\n",
       "      <td>en</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Imprisoned in the 1940s for the double murder ...</td>\n",
       "      <td>143.265</td>\n",
       "      <td>/9cqNxx0GxF0bflZmeSMuL5tnGzr.jpg</td>\n",
       "      <td>1994-09-23</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>False</td>\n",
       "      <td>8.705</td>\n",
       "      <td>26262</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>/tmU7GeKVybMWFButWEGl2M4GeiP.jpg</td>\n",
       "      <td>[18, 80]</td>\n",
       "      <td>238</td>\n",
       "      <td>en</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Spanning the years 1945 to 1955, a chronicle o...</td>\n",
       "      <td>132.599</td>\n",
       "      <td>/3bhkrj58Vtu7enYsRolD1fZdja1.jpg</td>\n",
       "      <td>1972-03-14</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>False</td>\n",
       "      <td>8.694</td>\n",
       "      <td>19913</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>/kGzFbGhp99zva6oZODW5atUtnqi.jpg</td>\n",
       "      <td>[18, 80]</td>\n",
       "      <td>240</td>\n",
       "      <td>en</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>In the continuing saga of the Corleone crime f...</td>\n",
       "      <td>119.635</td>\n",
       "      <td>/hek3koDUyRQk7FIhPXsa6mT2Zc3.jpg</td>\n",
       "      <td>1974-12-20</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>False</td>\n",
       "      <td>8.577</td>\n",
       "      <td>12026</td>\n",
       "      <td>Drama, Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>/zb6fM1CX41D9rF9hdgclu0peUmy.jpg</td>\n",
       "      <td>[18, 36, 10752]</td>\n",
       "      <td>424</td>\n",
       "      <td>en</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>The true story of how businessman Oskar Schind...</td>\n",
       "      <td>81.856</td>\n",
       "      <td>/sF1U4EUQS8YHUYjNl3pMGNIQyr0.jpg</td>\n",
       "      <td>1993-12-15</td>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>False</td>\n",
       "      <td>8.567</td>\n",
       "      <td>15434</td>\n",
       "      <td>Drama, History, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>/qqHQsStV6exghCM7zbObuYBiYxw.jpg</td>\n",
       "      <td>[18]</td>\n",
       "      <td>389</td>\n",
       "      <td>en</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>The defense and the prosecution have rested an...</td>\n",
       "      <td>83.144</td>\n",
       "      <td>/ow3wq89wM8qd5X7hWKxiRfsFf9C.jpg</td>\n",
       "      <td>1957-04-10</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>False</td>\n",
       "      <td>8.500</td>\n",
       "      <td>8295</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  adult                     backdrop_path        genre_ids   id  \\\n",
       "0           0  False  /zfbjgQE1uSd9wiPTX4VzsLi0rGG.jpg         [18, 80]  278   \n",
       "1           1  False  /tmU7GeKVybMWFButWEGl2M4GeiP.jpg         [18, 80]  238   \n",
       "2           2  False  /kGzFbGhp99zva6oZODW5atUtnqi.jpg         [18, 80]  240   \n",
       "3           3  False  /zb6fM1CX41D9rF9hdgclu0peUmy.jpg  [18, 36, 10752]  424   \n",
       "4           4  False  /qqHQsStV6exghCM7zbObuYBiYxw.jpg             [18]  389   \n",
       "\n",
       "  original_language            original_title  \\\n",
       "0                en  The Shawshank Redemption   \n",
       "1                en             The Godfather   \n",
       "2                en     The Godfather Part II   \n",
       "3                en          Schindler's List   \n",
       "4                en              12 Angry Men   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "0  Imprisoned in the 1940s for the double murder ...     143.265   \n",
       "1  Spanning the years 1945 to 1955, a chronicle o...     132.599   \n",
       "2  In the continuing saga of the Corleone crime f...     119.635   \n",
       "3  The true story of how businessman Oskar Schind...      81.856   \n",
       "4  The defense and the prosecution have rested an...      83.144   \n",
       "\n",
       "                        poster_path release_date                     title  \\\n",
       "0  /9cqNxx0GxF0bflZmeSMuL5tnGzr.jpg   1994-09-23  The Shawshank Redemption   \n",
       "1  /3bhkrj58Vtu7enYsRolD1fZdja1.jpg   1972-03-14             The Godfather   \n",
       "2  /hek3koDUyRQk7FIhPXsa6mT2Zc3.jpg   1974-12-20     The Godfather Part II   \n",
       "3  /sF1U4EUQS8YHUYjNl3pMGNIQyr0.jpg   1993-12-15          Schindler's List   \n",
       "4  /ow3wq89wM8qd5X7hWKxiRfsFf9C.jpg   1957-04-10              12 Angry Men   \n",
       "\n",
       "   video  vote_average  vote_count          genre_names  \n",
       "0  False         8.705       26262         Drama, Crime  \n",
       "1  False         8.694       19913         Drama, Crime  \n",
       "2  False         8.577       12026         Drama, Crime  \n",
       "3  False         8.567       15434  Drama, History, War  \n",
       "4  False         8.500        8295                Drama  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  df.to_csv('Movie_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['original_title', 'overview']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T11:36:49.725967Z",
     "start_time": "2024-06-13T11:36:49.643103Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m \n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PorterStemmer\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstem\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mporter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PorterStemmer\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import spacy\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T11:36:55.283870Z",
     "start_time": "2024-06-13T11:36:55.264923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/c4/g4th1y192hlgglxhh6s94n8m0000gn/T/ipykernel_42117/638079532.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  pattern = re.compile('https?://\\s+www\\.\\s+')\n",
      "/var/folders/c4/g4th1y192hlgglxhh6s94n8m0000gn/T/ipykernel_42117/638079532.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  pattern = re.compile('https?://\\s+www\\.\\s+')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 27\u001B[0m\n\u001B[1;32m     18\u001B[0m     emoji_pattern \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m                            \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\U0001F600\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;130;01m\\U0001F64F\u001B[39;00m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# emoticons\u001B[39;00m\n\u001B[1;32m     20\u001B[0m                            \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\U0001F300\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;130;01m\\U0001F5FF\u001B[39;00m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# symbols & pictographs\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     24\u001B[0m                            \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\U000024C2\u001B[39;00m\u001B[38;5;124m-\u001B[39m\u001B[38;5;130;01m\\U0001F251\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     25\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]+\u001B[39m\u001B[38;5;124m\"\u001B[39m, flags\u001B[38;5;241m=\u001B[39mre\u001B[38;5;241m.\u001B[39mUNICODE)\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m emoji_pattern\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[0;32m---> 27\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenise\u001B[39m(text):\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [token\u001B[38;5;241m.\u001B[39mtext \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m nlp(text)]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<..*?>')\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile('https?://\\s+www\\.\\s+')\n",
    "    return pattern.sub(r'', text)    \n",
    "\n",
    "def remove_punctuation(text) : \n",
    "    exclude = string.punctuation\n",
    "    return text.translate(str.maketrans('', '', exclude))\n",
    "    \n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "    \n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def tokenise(text):\n",
    "    return [token.text for token in nlp(text)]\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    punctuations = \"?:!.,;\"\n",
    "\n",
    "    sentence_words = word_tokenize(sentence)\n",
    "\n",
    "    sentence_words = [word for word in sentence_words if word not in punctuations]\n",
    "    \n",
    "    lemmatized_words = [(word, wordnet_lemmatizer.lemmatize(word, pos='v')) for word in sentence_words]\n",
    "\n",
    "    print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
    "    for word, lemma in lemmatized_words:\n",
    "        print(\"{0:20}{1:20}\".format(word, lemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].apply(remove_html_tag)\n",
    "df['overview'] = df['overview'].apply(remove_emoji)\n",
    "df['overview'] = df['overview'].apply(remove_punctuation)\n",
    "df['overview'] = df['overview'].apply(remove_html_tag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Spelling is not working or taking too long to complete this run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].apply(correct_spelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['overview'] = df['overview'].apply(tokenise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize Sectance is not Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/machine-learning/.venv/nltk_data'\n    - '/home/user/machine-learning/.venv/share/nltk_data'\n    - '/home/user/machine-learning/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m/home/user/machine-learning/src/Jupyter Folder/Gradient Decsent from Scratch.ipynb Cell 12\u001B[0m line \u001B[0;36m1\n\u001B[0;32m----> <a href='vscode-notebook-cell://idx-machine-learning-1716274113092.cluster-bec2e4635ng44w7ed22sa22hes.cloudworkstations.dev/home/user/machine-learning/src/Jupyter%20Folder/Gradient%20Decsent%20from%20Scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001B[0m new \u001B[39m=\u001B[39m df[\u001B[39m'\u001B[39;49m\u001B[39moverview\u001B[39;49m\u001B[39m'\u001B[39;49m]\u001B[39m.\u001B[39;49mapply(lemmatize_sentence)\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[39mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[39m*\u001B[39m\u001B[39m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[39m-\u001B[39m\u001B[39m>\u001B[39m DataFrame \u001B[39m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[39m    \u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[39m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[39m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[39mreturn\u001B[39;00m SeriesApply(\n\u001B[1;32m   4918\u001B[0m         \u001B[39mself\u001B[39;49m,\n\u001B[1;32m   4919\u001B[0m         func,\n\u001B[1;32m   4920\u001B[0m         convert_dtype\u001B[39m=\u001B[39;49mconvert_dtype,\n\u001B[1;32m   4921\u001B[0m         by_row\u001B[39m=\u001B[39;49mby_row,\n\u001B[1;32m   4922\u001B[0m         args\u001B[39m=\u001B[39;49margs,\n\u001B[1;32m   4923\u001B[0m         kwargs\u001B[39m=\u001B[39;49mkwargs,\n\u001B[0;32m-> 4924\u001B[0m     )\u001B[39m.\u001B[39;49mapply()\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[39m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mapply_standard()\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[39m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[39m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[39m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[39m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[39m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mignore\u001B[39m\u001B[39m\"\u001B[39m \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(obj\u001B[39m.\u001B[39mdtype, CategoricalDtype) \u001B[39melse\u001B[39;00m \u001B[39mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[39m=\u001B[39m obj\u001B[39m.\u001B[39;49m_map_values(\n\u001B[1;32m   1508\u001B[0m     mapper\u001B[39m=\u001B[39;49mcurried, na_action\u001B[39m=\u001B[39;49maction, convert\u001B[39m=\u001B[39;49m\u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mconvert_dtype\n\u001B[1;32m   1509\u001B[0m )\n\u001B[1;32m   1511\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mlen\u001B[39m(mapped) \u001B[39mand\u001B[39;00m \u001B[39misinstance\u001B[39m(mapped[\u001B[39m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[39m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[39mreturn\u001B[39;00m obj\u001B[39m.\u001B[39m_constructor_expanddim(\u001B[39mlist\u001B[39m(mapped), index\u001B[39m=\u001B[39mobj\u001B[39m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[39mreturn\u001B[39;00m arr\u001B[39m.\u001B[39mmap(mapper, na_action\u001B[39m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[39mreturn\u001B[39;00m algorithms\u001B[39m.\u001B[39;49mmap_array(arr, mapper, na_action\u001B[39m=\u001B[39;49mna_action, convert\u001B[39m=\u001B[39;49mconvert)\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[39m=\u001B[39m arr\u001B[39m.\u001B[39mastype(\u001B[39mobject\u001B[39m, copy\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[39mif\u001B[39;00m na_action \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[39mreturn\u001B[39;00m lib\u001B[39m.\u001B[39;49mmap_infer(values, mapper, convert\u001B[39m=\u001B[39;49mconvert)\n\u001B[1;32m   1744\u001B[0m \u001B[39melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[39mreturn\u001B[39;00m lib\u001B[39m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[39m=\u001B[39misna(values)\u001B[39m.\u001B[39mview(np\u001B[39m.\u001B[39muint8), convert\u001B[39m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "\u001B[1;32m/home/user/machine-learning/src/Jupyter Folder/Gradient Decsent from Scratch.ipynb Cell 12\u001B[0m line \u001B[0;36m4\n\u001B[1;32m     <a href='vscode-notebook-cell://idx-machine-learning-1716274113092.cluster-bec2e4635ng44w7ed22sa22hes.cloudworkstations.dev/home/user/machine-learning/src/Jupyter%20Folder/Gradient%20Decsent%20from%20Scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001B[0m wordnet_lemmatizer \u001B[39m=\u001B[39m WordNetLemmatizer()\n\u001B[1;32m     <a href='vscode-notebook-cell://idx-machine-learning-1716274113092.cluster-bec2e4635ng44w7ed22sa22hes.cloudworkstations.dev/home/user/machine-learning/src/Jupyter%20Folder/Gradient%20Decsent%20from%20Scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001B[0m punctuations \u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39m?:!.,;\u001B[39m\u001B[39m\"\u001B[39m\n\u001B[0;32m---> <a href='vscode-notebook-cell://idx-machine-learning-1716274113092.cluster-bec2e4635ng44w7ed22sa22hes.cloudworkstations.dev/home/user/machine-learning/src/Jupyter%20Folder/Gradient%20Decsent%20from%20Scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001B[0m sentence_words \u001B[39m=\u001B[39m word_tokenize(sentence)\n\u001B[1;32m     <a href='vscode-notebook-cell://idx-machine-learning-1716274113092.cluster-bec2e4635ng44w7ed22sa22hes.cloudworkstations.dev/home/user/machine-learning/src/Jupyter%20Folder/Gradient%20Decsent%20from%20Scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001B[0m sentence_words \u001B[39m=\u001B[39m [word \u001B[39mfor\u001B[39;00m word \u001B[39min\u001B[39;00m sentence_words \u001B[39mif\u001B[39;00m word \u001B[39mnot\u001B[39;00m \u001B[39min\u001B[39;00m punctuations]\n\u001B[1;32m     <a href='vscode-notebook-cell://idx-machine-learning-1716274113092.cluster-bec2e4635ng44w7ed22sa22hes.cloudworkstations.dev/home/user/machine-learning/src/Jupyter%20Folder/Gradient%20Decsent%20from%20Scratch.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001B[0m lemmatized_words \u001B[39m=\u001B[39m [(word, wordnet_lemmatizer\u001B[39m.\u001B[39mlemmatize(word, pos\u001B[39m=\u001B[39m\u001B[39m'\u001B[39m\u001B[39mv\u001B[39m\u001B[39m'\u001B[39m)) \u001B[39mfor\u001B[39;00m word \u001B[39min\u001B[39;00m sentence_words]\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, language, preserve_line)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39mword_tokenize\u001B[39m(text, language\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39menglish\u001B[39m\u001B[39m\"\u001B[39m, preserve_line\u001B[39m=\u001B[39m\u001B[39mFalse\u001B[39;00m):\n\u001B[1;32m    115\u001B[0m \u001B[39m    \u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[39m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[39m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[39m    :type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m     sentences \u001B[39m=\u001B[39m [text] \u001B[39mif\u001B[39;00m preserve_line \u001B[39melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[1;32m    130\u001B[0m     \u001B[39mreturn\u001B[39;00m [\n\u001B[1;32m    131\u001B[0m         token \u001B[39mfor\u001B[39;00m sent \u001B[39min\u001B[39;00m sentences \u001B[39mfor\u001B[39;00m token \u001B[39min\u001B[39;00m _treebank_word_tokenizer\u001B[39m.\u001B[39mtokenize(sent)\n\u001B[1;32m    132\u001B[0m     ]\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[39mdef\u001B[39;00m \u001B[39msent_tokenize\u001B[39m(text, language\u001B[39m=\u001B[39m\u001B[39m\"\u001B[39m\u001B[39menglish\u001B[39m\u001B[39m\"\u001B[39m):\n\u001B[1;32m     97\u001B[0m \u001B[39m    \u001B[39m\u001B[39m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[39m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[39m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[39m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[39m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 106\u001B[0m     tokenizer \u001B[39m=\u001B[39m load(\u001B[39mf\u001B[39;49m\u001B[39m\"\u001B[39;49m\u001B[39mtokenizers/punkt/\u001B[39;49m\u001B[39m{\u001B[39;49;00mlanguage\u001B[39m}\u001B[39;49;00m\u001B[39m.pickle\u001B[39;49m\u001B[39m\"\u001B[39;49m)\n\u001B[1;32m    107\u001B[0m     \u001B[39mreturn\u001B[39;00m tokenizer\u001B[39m.\u001B[39mtokenize(text)\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/nltk/data.py:750\u001B[0m, in \u001B[0;36mload\u001B[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[1;32m    747\u001B[0m     \u001B[39mprint\u001B[39m(\u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m<<Loading \u001B[39m\u001B[39m{\u001B[39;00mresource_url\u001B[39m}\u001B[39;00m\u001B[39m>>\u001B[39m\u001B[39m\"\u001B[39m)\n\u001B[1;32m    749\u001B[0m \u001B[39m# Load the resource.\u001B[39;00m\n\u001B[0;32m--> 750\u001B[0m opened_resource \u001B[39m=\u001B[39m _open(resource_url)\n\u001B[1;32m    752\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mformat\u001B[39m \u001B[39m==\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mraw\u001B[39m\u001B[39m\"\u001B[39m:\n\u001B[1;32m    753\u001B[0m     resource_val \u001B[39m=\u001B[39m opened_resource\u001B[39m.\u001B[39mread()\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/nltk/data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[0;34m(resource_url)\u001B[0m\n\u001B[1;32m    873\u001B[0m protocol, path_ \u001B[39m=\u001B[39m split_resource_url(resource_url)\n\u001B[1;32m    875\u001B[0m \u001B[39mif\u001B[39;00m protocol \u001B[39mis\u001B[39;00m \u001B[39mNone\u001B[39;00m \u001B[39mor\u001B[39;00m protocol\u001B[39m.\u001B[39mlower() \u001B[39m==\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mnltk\u001B[39m\u001B[39m\"\u001B[39m:\n\u001B[0;32m--> 876\u001B[0m     \u001B[39mreturn\u001B[39;00m find(path_, path \u001B[39m+\u001B[39;49m [\u001B[39m\"\u001B[39;49m\u001B[39m\"\u001B[39;49m])\u001B[39m.\u001B[39mopen()\n\u001B[1;32m    877\u001B[0m \u001B[39melif\u001B[39;00m protocol\u001B[39m.\u001B[39mlower() \u001B[39m==\u001B[39m \u001B[39m\"\u001B[39m\u001B[39mfile\u001B[39m\u001B[39m\"\u001B[39m:\n\u001B[1;32m    878\u001B[0m     \u001B[39m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n\u001B[1;32m    879\u001B[0m     \u001B[39mreturn\u001B[39;00m find(path_, [\u001B[39m\"\u001B[39m\u001B[39m\"\u001B[39m])\u001B[39m.\u001B[39mopen()\n",
      "File \u001B[0;32m~/machine-learning/.venv/lib/python3.11/site-packages/nltk/data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    581\u001B[0m sep \u001B[39m=\u001B[39m \u001B[39m\"\u001B[39m\u001B[39m*\u001B[39m\u001B[39m\"\u001B[39m \u001B[39m*\u001B[39m \u001B[39m70\u001B[39m\n\u001B[1;32m    582\u001B[0m resource_not_found \u001B[39m=\u001B[39m \u001B[39mf\u001B[39m\u001B[39m\"\u001B[39m\u001B[39m\\n\u001B[39;00m\u001B[39m{\u001B[39;00msep\u001B[39m}\u001B[39;00m\u001B[39m\\n\u001B[39;00m\u001B[39m{\u001B[39;00mmsg\u001B[39m}\u001B[39;00m\u001B[39m\\n\u001B[39;00m\u001B[39m{\u001B[39;00msep\u001B[39m}\u001B[39;00m\u001B[39m\\n\u001B[39;00m\u001B[39m\"\u001B[39m\n\u001B[0;32m--> 583\u001B[0m \u001B[39mraise\u001B[39;00m \u001B[39mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/home/user/nltk_data'\n    - '/home/user/machine-learning/.venv/nltk_data'\n    - '/home/user/machine-learning/.venv/share/nltk_data'\n    - '/home/user/machine-learning/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "new = df['overview'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
